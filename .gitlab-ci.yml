.build_container_image:
  image:
    name: gcr.io/kaniko-project/executor:debug
    entrypoint: [ '' ]
  variables:
    DOCKERFILE_LOCATION: 'unset'
    CONTAINER_NAME: 'unset'
    TAG_NAME: 'unset'
  script:
    - mkdir -p /kaniko/.docker
    - echo "DOCKERFILE_LOCATION" $DOCKERFILE_LOCATION
    - echo "CONTAINER_NAME" $CONTAINER_NAME
    - echo "TAG_NAME" $TAG_NAME
    - echo "{\"auths\":{\"$CI_REGISTRY\":{\"username\":\"$CI_REGISTRY_USER\",\"password\":\"$CI_REGISTRY_PASSWORD\"}}}" > /kaniko/.docker/config.json
    - |
      if [[ "$CREATE_LATEST_TAG" == "true" ]];
      then
      /kaniko/executor --context $CI_PROJECT_DIR --cache=true --dockerfile $DOCKERFILE_LOCATION --destination $CI_REGISTRY_IMAGE/$CONTAINER_NAME:$TAG_NAME --destination    $CI_REGISTRY_IMAGE/$CONTAINER_NAME:latest 
      else
      /kaniko/executor --context $CI_PROJECT_DIR --cache=true --dockerfile $DOCKERFILE_LOCATION --destination $CI_REGISTRY_IMAGE/$CONTAINER_NAME:$TAG_NAME;
      fi

stages:
  - print_readme
  - commit
  - acceptance
  - release

##################################################
# Tasks of "generate"- Stage
##################################################
print_readme:
  stage: print_readme
  script:
    - echo + Sonarqube Link "$sonarqube_link" +  Sonarqube Username "$sonarqube_user" + Sonarqube Password "$sonarqube_pass"
    - echo + Graylog Link "$graylog_link" +  Graylog Username "$graylog_user" + Graylog Password "$graylog_pass"
    - echo + Kubernetes Dashboard Link "$dashboard_link" +  Kubernetes Dashboard Token "$dashboard_token"
  only:
    - branches
    - merge_requests
    - main
    - master

variables:
  DEVELOPMENT_CONTAINER_NAME: 'development-container'
  DEVELOPMENT_CONTAINER_TAG: "${CI_COMMIT_BRANCH}_${CI_COMMIT_SHORT_SHA}"
  RELEASE_CONTAINER_NAME: 'release-container'
  RELEASE_CONTAINER_TAG: "${CI_COMMIT_BRANCH}_${CI_COMMIT_SHORT_SHA}"


create_development_container_image:
  stage: commit
  extends:
    - .build_container_image
  variables:
    DOCKERFILE_LOCATION: $CI_PROJECT_DIR/infra/build_artifacts/development.dockerfile
    CONTAINER_NAME: $DEVELOPMENT_CONTAINER_NAME
    TAG_NAME: $DEVELOPMENT_CONTAINER_TAG
    CREATE_LATEST_TAG: 'true'
  only:
    - branches
    - merge_requests
    - main
    - master
  except:
    - schedules

flake8_checking:
  stage: commit
  needs:
    - create_development_container_image
  image:
    name: $CI_REGISTRY_IMAGE/$DEVELOPMENT_CONTAINER_NAME:$DEVELOPMENT_CONTAINER_TAG
    entrypoint: [ "" ]
  script:
    - flakeheaven lint app/ tests/
  allow_failure: true
  only:
    - branches
    - merge_requests
    - main
    - master
  except:
    - schedules

create_release_candidate:
  stage: acceptance
  extends:
    - .build_container_image
  variables:
    DOCKERFILE_LOCATION: $CI_PROJECT_DIR/infra/build_artifacts/release.dockerfile
    CONTAINER_NAME: $RELEASE_CONTAINER_NAME
    TAG_NAME: $RELEASE_CONTAINER_TAG
    CREATE_LATEST_TAG: 'true'
  only:
    - merge_requests
    - main
    - master
  except:
    - schedules

deploy_application_to_staging:
  stage: acceptance
  needs:
    - create_release_candidate
  image:
    name: dtzar/helm-kubectl:3.10.3
    entrypoint: [ '' ]
  before_script:
    - cp infra/deployment/deployment-staging.yaml final-deployment-staging.yaml
    - sed -i "s|enter_release_containter_registry_url|$CI_REGISTRY_IMAGE/$RELEASE_CONTAINER_NAME:latest|g" final-deployment-staging.yaml
    - cp infra/deployment/kubeconfig final-kubeconfig
    - sed -i "s|enter_instace_url|$team_instance_link|g" final-kubeconfig
    - sed -i "s|enter_kubernetes_token|$dashboard_token|g" final-kubeconfig
  script:
    - kubectl delete deployment stagingcontainer --ignore-not-found=true --namespace staging --kubeconfig=final-kubeconfig
    - kubectl delete service stagingcontainer-service --ignore-not-found=true --namespace staging  --kubeconfig=final-kubeconfig
    - kubectl apply -f final-deployment-staging.yaml --kubeconfig=final-kubeconfig
    - rm final-deployment-staging.yaml
    - rm final-kubeconfig
  only:
    refs:
      - main
      - merge_requests
      - master
  when: manual
  except:
    - schedules

smoke_testing_staging:
  stage: acceptance
  variables:
    API_SERVER: $team_instance_link
    API_PORT: 30080
  needs:
    - deploy_application_to_staging
  before_script:
    - apt-get update && apt-get install -y curl
  script:
    - chmod +x $CI_PROJECT_DIR/infra/build_artifacts/smoke.sh
    - $CI_PROJECT_DIR/infra/build_artifacts/smoke.sh
  only:
    - merge_requests
    - main
    - master
  except:
    - schedules

migrate_database_in_staging:
  stage: acceptance
  needs:
    - smoke_testing_staging
  image:
    name: dtzar/helm-kubectl:3.10.3
    entrypoint: [ '' ]
  before_script:
    - cp infra/deployment/kubeconfig final-kubeconfig
    - sed -i "s|enter_instace_url|$team_instance_link|g" final-kubeconfig
    - sed -i "s|enter_kubernetes_token|$dashboard_token|g" final-kubeconfig
  script:
    - export POD_NAME=`kubectl --kubeconfig=final-kubeconfig get pods --namespace staging --no-headers -l app=stagingcontainer -o custom-columns=":metadata.name"`
    - kubectl --kubeconfig=final-kubeconfig exec $POD_NAME --namespace staging -- env PYTHONPATH=. alembic upgrade head
    - rm final-kubeconfig
  only:
    refs:
      - main
      - merge_requests
      - master
  except:
    - schedules

deploy_application_to_production:
  stage: release
  image: dtzar/helm-kubectl:3.10.3
  before_script:
    - cp infra/deployment/deployment-production.yaml final-deployment-production.yaml
    - sed -i "s|enter_release_containter_registry_url|$CI_REGISTRY_IMAGE/$RELEASE_CONTAINER_NAME:latest|g" final-deployment-production.yaml
    - cp infra/deployment/kubeconfig final-kubeconfig
    - sed -i "s|enter_instace_url|$team_instance_link|g" final-kubeconfig
    - sed -i "s|enter_kubernetes_token|$dashboard_token|g" final-kubeconfig
  script:
    - kubectl delete deployment productioncontainer --ignore-not-found=true --namespace production --kubeconfig=final-kubeconfig
    - kubectl delete service productioncontainer-service --ignore-not-found=true --namespace production  --kubeconfig=final-kubeconfig
    - kubectl apply -f final-deployment-production.yaml --kubeconfig=final-kubeconfig
    - rm final-deployment-production.yaml
    - rm final-kubeconfig
  only:
    refs:
      - main
      - master
  when: manual
  except:
    - schedules

smoke_testing_production:
  stage: release
  variables:
    API_SERVER: $team_instance_link
    API_PORT: 30081
  needs:
    - deploy_application_to_production
  before_script:
    - apt-get update && apt-get install -y curl
  script:
    - chmod +x $CI_PROJECT_DIR/infra/build_artifacts/smoke.sh
    - $CI_PROJECT_DIR/infra/build_artifacts/smoke.sh
  only:
    - main
    - master
  except:
    - schedules

migrate_database_in_production:
  stage: release
  needs:
    - smoke_testing_production
  image:
    name: dtzar/helm-kubectl:3.10.3
    entrypoint: [ '' ]
  before_script:
    - cp infra/deployment/kubeconfig final-kubeconfig
    - sed -i "s|enter_instace_url|$team_instance_link|g" final-kubeconfig
    - sed -i "s|enter_kubernetes_token|$dashboard_token|g" final-kubeconfig
  script:
    - export POD_NAME=`kubectl --kubeconfig=final-kubeconfig get pods --namespace production --no-headers -l app=productioncontainer -o custom-columns=":metadata.name"`
    - kubectl --kubeconfig=final-kubeconfig exec $POD_NAME --namespace production -- env PYTHONPATH=. alembic upgrade head
    - rm final-kubeconfig
  only:
    refs:
      - main
      - master
  except:
    - schedules